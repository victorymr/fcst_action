{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fcst_campaign core module\n",
    "\n",
    "> Gets forecast from BigQuery, loads campaigns and get historic launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "credentials_path = r'C:\\Users\\vjman\\Documents\\Projects\\Code\\keys\\climadatutorial-051f2156cfbb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    !pip install timezonefinder --quiet\n",
    "    !pip install google-cloud-bigquery --quiet\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive') #,force_remount=True)\n",
    "    \n",
    "    from google.colab import userdata\n",
    "    credentials_path = userdata.get('climada_cred_path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if IN_COLAB:\n",
    "  !pip install git+https://github.com/pete88b/nbdev_colab_helper.git --quiet\n",
    "  from nbdev_colab_helper.core import *\n",
    "  project_name = 'fcst_action'\n",
    "  init_notebook(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat_lon': (40.7128, -74.006), 'time_zone': 'America/New_York', 'local_peak_start': '2024-03-21 10:00:00', 'local_peak_end': '2024-03-21 17:00:00'}\n",
      "{'lat_lon': (28.6139, 77.209), 'time_zone': 'Asia/Kolkata', 'local_peak_start': '2024-03-21 19:30:00', 'local_peak_end': '2024-03-22 02:30:00'}\n",
      "Timezone offset for latitude 40.7128, longitude -74.006 is -4.0 hours.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from fcst_action.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pytz\n",
    "import sys\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import gspread_dataframe\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "from google.auth import default\n",
    "\n",
    "def get_campaign_details(filn=\"WeatherAdCampaign\",credentials_path=credentials_path):\n",
    "  #creds, _ = default()\n",
    "  #gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
    "  #creds = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "  # Use service account credentials with explicit scope\n",
    "  scopes = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "  creds = service_account.Credentials.from_service_account_file(credentials_path, scopes=scopes)\n",
    "  gc = gspread.authorize(creds)\n",
    "  workbook = gc.open(filn)\n",
    "\n",
    "  # Reading the 'Company' worksheet\n",
    "  company_df = get_as_dataframe(workbook.worksheet('Company'), evaluate_formulas=True, usecols=lambda x: x not in [''], dtype=str).dropna(how='all').dropna(axis=1, how='all')\n",
    "  companies = list(company_df['Company'].unique())\n",
    "\n",
    "  # Reading the 'Campaigns' worksheet\n",
    "  campaigns_df = get_as_dataframe(workbook.worksheet('Campaigns'), evaluate_formulas=True, usecols=lambda x: x not in [''], dtype=str).dropna(how='all').dropna(axis=1, how='all')\n",
    "  ## campaigns_df data is in F, convert to C since the BigQuery is in C\n",
    "  campaigns_df[['Tmax','Tmin']] = (campaigns_df[['Tmax','Tmin']].astype(float)-32)*5/9\n",
    "  return company_df, campaigns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fetch_forecast_data_all(city_lat, city_lon, start_date, end_date, project_id, credentials_path):\n",
    "    # Construct a BigQuery client object.\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH daily_forecasts AS (\n",
    "      SELECT\n",
    "        creation_time,\n",
    "        DATE(forecast.time) AS forecast_date,\n",
    "        MAX(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS max_temp,\n",
    "        MIN(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS min_temp,\n",
    "        AVG(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS avg_temp,\n",
    "        SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0)) AS total_precipitation\n",
    "      FROM\n",
    "        `bigquery-public-data.noaa_global_forecast_system.NOAA_GFS0P25`,\n",
    "        UNNEST(forecast) AS forecast\n",
    "      WHERE\n",
    "        creation_time BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND ST_DWithin(geography, ST_GeogPoint({city_lon}, {city_lat}), 5000)\n",
    "      GROUP BY\n",
    "        creation_time,\n",
    "        forecast_date\n",
    "    )\n",
    "    SELECT\n",
    "      creation_time,\n",
    "      forecast_date,\n",
    "      max_temp,\n",
    "      min_temp,\n",
    "      avg_temp,\n",
    "      total_precipitation\n",
    "    FROM\n",
    "      daily_forecasts\n",
    "    ORDER BY\n",
    "      creation_time,\n",
    "      forecast_date\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query)  # Make an API request.\n",
    "    results = query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = results.to_dataframe()\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "project_id = 'climadatutorial'\n",
    "#credentials_path = '/content/drive/Othercomputers/My PC (1)/Documents/Projects/Code/keys/climadatutorial-051f2156cfbb.json'\n",
    "\n",
    "start_date = '2023-07-01'\n",
    "end_date = '2023-08-31'\n",
    "\n",
    "#df = fetch_forecast_data(city_lat, city_lon, start_date, end_date, project_id, credentials_path)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fetch_forecast_data_lead(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days=3):\n",
    "    # Construct a BigQuery client object.\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "    offset_hour = int(get_timezone_offset(city_lat, city_lon))  # Ensure this function call is correct\n",
    "    print(offset_hour)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH daily_forecasts AS (\n",
    "      SELECT\n",
    "        creation_time,\n",
    "        DATE(DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR)) AS local_forecast_date,\n",
    "        MAX(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS max_temp,\n",
    "        MIN(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS min_temp,\n",
    "        AVG(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS avg_temp,\n",
    "        SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0)) AS total_precipitation,\n",
    "        AVG(IF(TIME(DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR)) BETWEEN '10:00:00' AND '17:00:00' AND forecast.total_cloud_cover_entire_atmosphere IS NOT NULL, forecast.total_cloud_cover_entire_atmosphere, NULL)) AS avg_cloud_cover,\n",
    "        CASE\n",
    "          WHEN AVG(forecast.temperature_2m_above_ground) < 32 THEN SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0))\n",
    "          ELSE 0\n",
    "        END AS total_snow,\n",
    "        CASE\n",
    "          WHEN AVG(forecast.temperature_2m_above_ground) >= 32 THEN SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0))\n",
    "          ELSE 0\n",
    "        END AS total_rain\n",
    "      FROM\n",
    "        `bigquery-public-data.noaa_global_forecast_system.NOAA_GFS0P25`,\n",
    "        UNNEST(forecast) AS forecast\n",
    "      WHERE\n",
    "        creation_time BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND ST_DWithin(geography, ST_GeogPoint({city_lon}, {city_lat}), 5000)\n",
    "        AND DATE(forecast.time) = DATE_ADD(DATE(creation_time), INTERVAL {lead_days} DAY)\n",
    "      GROUP BY\n",
    "        creation_time,\n",
    "        local_forecast_date\n",
    "    )\n",
    "    SELECT\n",
    "      creation_time,\n",
    "      local_forecast_date AS forecast_date,\n",
    "      max_temp,\n",
    "      min_temp,\n",
    "      avg_temp,\n",
    "      total_precipitation,\n",
    "      avg_cloud_cover,\n",
    "      total_snow,\n",
    "      total_rain\n",
    "    FROM\n",
    "      daily_forecasts\n",
    "    ORDER BY\n",
    "      creation_time,\n",
    "      local_forecast_date\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query)  # Make an API request.\n",
    "    results = query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = results.to_dataframe()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def fetch_forecast_data_lead2(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days=3,round2grid=1):\n",
    "    # Assuming offset_hour is correctly calculated earlier in the function\n",
    "    offset_hour = int(get_timezone_offset(city_lat, city_lon))  # Placeholder for actual timezone offset calculation\n",
    "    # Construct a BigQuery client object.\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "    # If you decided to round the lat,lon to the grid of the source data\n",
    "    # this increases the chance of matching - but the rounding might create error\n",
    "    # and you need to confirm the grid resolution - default is 0.25,0.25\n",
    "    city_lat, city_lon =  round_to_grid(city_lat, city_lon)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH forecast_adjustments AS (\n",
    "      SELECT\n",
    "        creation_time,\n",
    "        forecast.time AS utc_forecast_time,\n",
    "        DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR) AS local_forecast_time,\n",
    "        DATE(DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR)) AS local_forecast_date,\n",
    "        forecast.temperature_2m_above_ground,\n",
    "        forecast.relative_humidity_2m_above_ground,\n",
    "        forecast.total_precipitation_surface,\n",
    "        forecast.total_cloud_cover_entire_atmosphere\n",
    "      FROM\n",
    "        `bigquery-public-data.noaa_global_forecast_system.NOAA_GFS0P25`,\n",
    "        UNNEST(forecast) AS forecast\n",
    "      WHERE\n",
    "        creation_time BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND ST_DWithin(geography, ST_GeogPoint({city_lon}, {city_lat}), 5000)\n",
    "    ),\n",
    "    target_forecasts AS (\n",
    "      SELECT\n",
    "        creation_time,\n",
    "        local_forecast_date,\n",
    "        local_forecast_time,\n",
    "        temperature_2m_above_ground,\n",
    "        relative_humidity_2m_above_ground,\n",
    "        total_precipitation_surface,\n",
    "        total_cloud_cover_entire_atmosphere,\n",
    "        DATE(DATETIME_ADD(creation_time, INTERVAL {lead_days * 24} HOUR)) AS target_date\n",
    "      FROM forecast_adjustments\n",
    "    ),\n",
    "    day_aggregates AS (\n",
    "      SELECT\n",
    "        creation_time,\n",
    "        local_forecast_date,\n",
    "        MAX(temperature_2m_above_ground) AS max_temp,\n",
    "        MIN(temperature_2m_above_ground) AS min_temp,\n",
    "        AVG(temperature_2m_above_ground) AS avg_temp,\n",
    "        AVG(relative_humidity_2m_above_ground) AS avg_rh,\n",
    "        SUM(total_precipitation_surface) AS total_precipitation,\n",
    "        SUM(CASE WHEN temperature_2m_above_ground < 0 THEN total_precipitation_surface ELSE 0 END) AS total_snow,\n",
    "        SUM(CASE WHEN temperature_2m_above_ground >= 0 THEN total_precipitation_surface ELSE 0 END) AS total_rain,\n",
    "        AVG(CASE WHEN EXTRACT(HOUR FROM local_forecast_time) BETWEEN 10 AND 17 THEN total_cloud_cover_entire_atmosphere ELSE NULL END) AS avg_cloud_cover\n",
    "      FROM target_forecasts\n",
    "      WHERE local_forecast_date = target_date\n",
    "      GROUP BY creation_time, local_forecast_date\n",
    "    )\n",
    "    SELECT\n",
    "      creation_time,\n",
    "      local_forecast_date,\n",
    "      max_temp,\n",
    "      min_temp,\n",
    "      avg_temp,\n",
    "      avg_rh,\n",
    "      total_precipitation,\n",
    "      total_snow,\n",
    "      total_rain,\n",
    "      avg_cloud_cover\n",
    "    FROM day_aggregates\n",
    "    ORDER BY creation_time, local_forecast_date\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query)  # Make an API request.\n",
    "    results = query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = results.to_dataframe()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fetch_precip_forecast_data_lead2(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days=3,round2grid=1):\n",
    "    # Separating out the extraction of precip data and then merging it later - since precip has a different accumulation in GFS0P25\n",
    "    # Assuming offset_hour is correctly calculated earlier in the function\n",
    "    offset_hour = int(get_timezone_offset(city_lat, city_lon))  # Placeholder for actual timezone offset calculation\n",
    "    # Construct a BigQuery client object.\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "    # If you decided to round the lat,lon to the grid of the source data\n",
    "    # this increases the chance of matching - but the rounding might create error\n",
    "    # and you need to confirm the grid resolution - default is 0.25,0.25\n",
    "    city_lat, city_lon =  round_to_grid(city_lat, city_lon)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH precipitation_and_temperature_data AS (\n",
    "    SELECT\n",
    "        creation_time,\n",
    "        forecast.time AS utc_forecast_time,\n",
    "        DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR) AS local_forecast_time,\n",
    "        DATE(DATETIME_ADD(forecast.time, INTERVAL {offset_hour} HOUR)) AS local_forecast_date,\n",
    "        forecast.total_precipitation_surface,\n",
    "        forecast.temperature_2m_above_ground,\n",
    "        forecast.hours,\n",
    "        DATE(DATETIME_ADD(creation_time, INTERVAL {lead_days} DAY)) AS target_forecast_date,\n",
    "        MOD(forecast.hours, 6) AS forecast_hours_mod_6 -- Calculate the modulo here\n",
    "    FROM\n",
    "        `bigquery-public-data.noaa_global_forecast_system.NOAA_GFS0P25`,\n",
    "        UNNEST(forecast) AS forecast\n",
    "    WHERE\n",
    "        creation_time BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND ST_DWithin(geography, ST_GeogPoint({city_lon}, {city_lat}), 5000)\n",
    "    ),\n",
    "    filtered_precipitation_and_temperature AS (\n",
    "    SELECT\n",
    "        creation_time,\n",
    "        local_forecast_date,\n",
    "        total_precipitation_surface,\n",
    "        temperature_2m_above_ground,\n",
    "        hours,\n",
    "        target_forecast_date,\n",
    "    FROM precipitation_and_temperature_data\n",
    "    WHERE forecast_hours_mod_6 = 0 AND local_forecast_date = target_forecast_date\n",
    "    ),\n",
    "    aggregated_precipitation AS (\n",
    "    SELECT\n",
    "        creation_time,\n",
    "        local_forecast_date,\n",
    "        SUM(total_precipitation_surface) AS total_daily_precipitation,\n",
    "        SUM(CASE WHEN temperature_2m_above_ground < 0 THEN total_precipitation_surface ELSE 0 END) AS total_snow,\n",
    "        SUM(CASE WHEN temperature_2m_above_ground >= 0 THEN total_precipitation_surface ELSE 0 END) AS total_rain\n",
    "    FROM filtered_precipitation_and_temperature\n",
    "    GROUP BY creation_time, local_forecast_date\n",
    "    )\n",
    "    SELECT\n",
    "        creation_time,\n",
    "        local_forecast_date,\n",
    "        total_daily_precipitation,\n",
    "        total_snow,\n",
    "        total_rain\n",
    "    FROM aggregated_precipitation\n",
    "    ORDER BY creation_time, local_forecast_date\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query)  # Make an API request.\n",
    "    results = query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = results.to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fetch_forecast_data_lead3(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days=3,round2grid=1):\n",
    "    ## Get the precip and the other forecast data separately for the lead days\n",
    "    forecast_df = fetch_forecast_data_lead2(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days,round2grid)\n",
    "    ##\n",
    "    precip_df = fetch_precip_forecast_data_lead2(city_lat, city_lon, start_date, end_date, project_id, credentials_path, lead_days,round2grid)\n",
    "\n",
    "    ## Merge the two dataframes\n",
    "    forecast_df.drop(['total_precipitation','total_snow','total_rain'],axis=1, inplace=True)\n",
    "    df = pd.merge(forecast_df, precip_df, on=['creation_time', 'local_forecast_date'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_campaign_trigger(forecast_row, campaign_row):\n",
    "    # Temperature criteria\n",
    "    temp_triggered = True  # Default to True if no temperature criteria\n",
    "    if pd.notna(campaign_row['Tmin']):\n",
    "        temp_triggered &= (forecast_row['max_temp'] >= float(campaign_row['Tmin']))\n",
    "    if pd.notna(campaign_row['Tmax']):\n",
    "        temp_triggered &= (forecast_row['max_temp'] <= float(campaign_row['Tmax']))\n",
    "\n",
    "    # Precipitation criteria\n",
    "    precip_triggered = True  # Default to True if no precipitation criteria\n",
    "    if pd.notna(campaign_row['Pmin']):\n",
    "        precip_triggered &= (forecast_row['total_precipitation'] >= float(campaign_row['Pmin']))\n",
    "    if pd.notna(campaign_row['Pmax']):\n",
    "        precip_triggered &= (forecast_row['total_precipitation'] <= float(campaign_row['Pmax']))\n",
    "\n",
    "    # Snow criteria\n",
    "    snow_triggered = True  # Default to True if no precipitation criteria\n",
    "    if pd.notna(campaign_row['Snowmin']):\n",
    "        snow_triggered &= (forecast_row['total_snow'] >= float(campaign_row['Snowmin']))\n",
    "    if pd.notna(campaign_row['Snowmax']):\n",
    "        precip_triggered &= (forecast_row['total_snow'] <= float(campaign_row['Snowmax']))\n",
    "\n",
    "    # Relative Humidity (RHmax) criteria\n",
    "    # Assuming RHmax means the campaign triggers if RH is less than RHmax\n",
    "    rh_triggered = True  # Default to True if no RH criteria\n",
    "    if pd.notna(campaign_row['RHmin']):\n",
    "        # Placeholder for RH comparison, assuming you have RH data\n",
    "        rh_triggered &= (forecast_row['avg_rh'] > float(campaign_row['RHmin']))  # Placeholder for actual RH data\n",
    "    if pd.notna(campaign_row['RHmax']):\n",
    "        # Placeholder for RH comparison, assuming you have RH data\n",
    "        rh_triggered &= (forecast_row['avg_rh'] <= float(campaign_row['RHmax']))  # Placeholder for actual RH data\n",
    "\n",
    "    # Sunlight (Sun) criteria\n",
    "    # Assuming Sun criteria means a certain threshold of sunlight, like avg_cloud_cover < threshold\n",
    "    sun_triggered = True  # Default to True if no Sun criteria\n",
    "    if pd.notna(campaign_row['Sun']):\n",
    "        # Placeholder for sunlight comparison\n",
    "        sun_triggered &= (forecast_row['avg_cloud_cover'] < float(campaign_row['Sun']))  # Assuming lower cloud cover means more sunlight\n",
    "\n",
    "    # Combine all criteria\n",
    "    campaign_triggered = temp_triggered & precip_triggered & rh_triggered & sun_triggered & snow_triggered\n",
    "    return campaign_triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def assign_ad_type(forecast_row, campaigns_df):\n",
    "  # Function to assign AdType to each forecast entry based on the first triggered campaign\n",
    "\n",
    "  for _, campaign_row in campaigns_df.iterrows():\n",
    "      if evaluate_campaign_trigger(forecast_row, campaign_row):\n",
    "          return campaign_row['AdType']\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_campaign_story(comp_row):\n",
    "    story_string = comp_row['Company'] +' is in the ' + comp_row['Product'] + ' business operating in ' + comp_row['Geo']\n",
    "    add_background = \"Here is some additional background\"\n",
    "    print(story_string)\n",
    "    print(add_background)\n",
    "    print(comp_row['Story'])\n",
    "    return [story_string,add_background, comp_row['Story']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_campaign_hist(company_df, campaigns_df, company_sel = 'BigAssFans',lead_days=3,recent_days=0):\n",
    "    ## Do this for a company, consolidate campaign per city\n",
    "    \n",
    "    comp_row = company_df[company_df['Company']==company_sel].iloc[-1,:]  # picking the last entry of this company in the df - incase there are multiple entries\n",
    "    camp_sel = campaigns_df[campaigns_df['Company']==company_sel]\n",
    "    lat_lon_list,city_names = create_geo_query_list(company_df, company_sel)\n",
    "    city_locations = dict(zip(city_names,lat_lon_list))\n",
    "    ##The below assumes all campaigns in a company have the same start and end date..fixing it would require surgery\n",
    "    if recent_days > 0:\n",
    "        end_date = datetime.datetime.now()\n",
    "        start_date = end_date - datetime.timedelta(days=recent_days)\n",
    "    else:\n",
    "        start_date,end_date = datetime.strptime(camp_sel['start_date'].iloc[0],'%m/%d/%Y'),datetime.strptime(camp_sel['end_date'].iloc[0],'%m/%d/%Y')\n",
    "    ##\n",
    "    # out_story = print_campaign_story(comp_row)\n",
    "    \n",
    "    # Initialize city_res_df with creation_time from forecast_df of the first city as a starting point\n",
    "    first_city_lat, first_city_lon = lat_lon_list[0]\n",
    "    first_forecast_df = fetch_forecast_data_lead2(first_city_lat, first_city_lon, start_date, end_date, project_id, credentials_path, lead_days=lead_days)\n",
    "    city_res_df = pd.DataFrame(first_forecast_df['creation_time']).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    for ((city_lat,city_lon),city_name) in zip(lat_lon_list,city_names):\n",
    "        forecast_df = fetch_forecast_data_lead2(city_lat, city_lon, start_date, end_date, project_id, credentials_path,lead_days=lead_days)\n",
    "        # Example application of the function\n",
    "        # Assuming campaigns_df is already filtered for a specific company\n",
    "        forecast_df['AdType'] = forecast_df.apply(assign_ad_type, axis=1, campaigns_df=camp_sel)\n",
    "        # Merge on 'creation_time' to align data\n",
    "        merged_df = city_res_df.merge(forecast_df[['creation_time', 'AdType']], on='creation_time', how='left')\n",
    "        merged_df.rename(columns={'AdType': city_name}, inplace=True)\n",
    "        city_res_df = merged_df\n",
    "\n",
    "    return city_res_df, city_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def campaign_heatmap(city_res_df):\n",
    "  # Assuming city_res_df is your DataFrame\n",
    "\n",
    "  # Convert 'creation_time' to a datetime type if it's not already\n",
    "  city_res_df['creation_time'] = pd.to_datetime(city_res_df['creation_time']).dt.strftime('%Y-%m-%dT%HZ')\n",
    "\n",
    "  # Melt the DataFrame to long format for easier plotting with seaborn\n",
    "  df_long = city_res_df.melt(id_vars=['creation_time'], var_name='City', value_name='AdType')\n",
    "\n",
    "  # Remove rows where AdType is None\n",
    "  df_long = df_long.dropna(subset=['AdType'])\n",
    "\n",
    "  # Create a heat map\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  heatmap = sns.heatmap(data=pd.pivot_table(df_long, values='AdType', index=['City'], columns=['creation_time'], aggfunc=lambda x: 1),\n",
    "                        cmap=\"crest\", cbar=False)\n",
    "\n",
    "  heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "  #plt.xticks(locator=plt.MaxNLocator(5))\n",
    "  plt.locator_params(axis='x', nbins=20)\n",
    "  plt.title('Ad Display Schedule by City and Time')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_map(data):\n",
    "  # Create a map for this time step\n",
    "  m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n",
    "  # Add markers to map using GeoJSON\n",
    "  folium.GeoJson(data, name=\"Ad Campaigns\").add_to(m)\n",
    "  return m\n",
    "\n",
    "data = {\n",
    "  'type': 'FeatureCollection',\n",
    "  'features': []\n",
    "}\n",
    "\n",
    "def map_campaigns_by_day(city_res_df,city_locations,suff=''):\n",
    "  ## Map of selected day and the various campaigns to be run..\n",
    "  # Ensure 'creation_time' includes the date and hour in ISO 8601 format\n",
    "  city_res_df['creation_time'] = pd.to_datetime(city_res_df['creation_time']).dt.strftime('%Y-%m-%dT%H:00:00Z')\n",
    "  import folium\n",
    "  from folium.plugins import TimestampedGeoJson\n",
    "\n",
    "  '''\n",
    "  # Define city locations (replace with your actual data)\n",
    "  city_locations = {\n",
    "      'Washington, D.C.': (38.9072, -77.0369),\n",
    "      'Austin, TX': (30.2672, -97.7431),\n",
    "      'Sacramento, CA': (38.5816, -121.4944),\n",
    "      'New York, NY': (40.7128, -74.0060),\n",
    "      'Chicago, IL': (41.8781, -87.6298),\n",
    "      'Denver, CO': (39.7392, -104.9903)\n",
    "  }\n",
    "  '''\n",
    "  # Existing loop to create features\n",
    "  for index, row in city_res_df.iterrows():\n",
    "    for city, ad_type in row.items():\n",
    "      if city == 'creation_time':\n",
    "        continue  # Skip the creation_time column itself\n",
    "      if city in city_locations:\n",
    "        feature = {\n",
    "          'type': 'Feature',\n",
    "          'geometry': {\n",
    "            'type': 'Point',\n",
    "            'coordinates': [city_locations[city][1], city_locations[city][0]],\n",
    "          },\n",
    "          'properties': {\n",
    "            'time': row['creation_time'],  # Now includes the hour\n",
    "            'ad_type': ad_type,  # This line is added\n",
    "            'icon': 'circle',\n",
    "            'iconstyle': {\n",
    "              'fillColor': '#ffffff' if ad_type is None else '#0078A8',  # White for None, blue otherwise\n",
    "              'fillOpacity': 1,  # Always fully opaque\n",
    "              'stroke': 'true',\n",
    "              'radius': 7,\n",
    "            },\n",
    "            'popup': city,\n",
    "          },\n",
    "        }\n",
    "        data['features'].append(feature)\n",
    "\n",
    "  # Create a map for the first time step and add TimestampedGeoJson\n",
    "  map = create_map(data.copy())  # Copy data to avoid modification\n",
    "  ts_geojson = TimestampedGeoJson(data, period='PT6H', add_last_point=False).add_to(map)\n",
    "\n",
    "  # Save the map as HTML\n",
    "  outfiln = 'ads_over_time'+suff+'.html'\n",
    "  map.save(outfiln)\n",
    "  print(f\"Map saved to {outfiln}\")\n",
    "\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show some summary stats of # of Ads of each type by City, and a time series by city\n",
    "## of days campaign is run and % of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_campaign_summary(city_res_df):\n",
    "  # Assuming city_res_df is your DataFrame and it's already sorted by 'creation_time'\n",
    "  city_res_df['creation_time'] = pd.to_datetime(city_res_df['creation_time'])\n",
    "\n",
    "  # Calculate the time difference between consecutive entries\n",
    "  #city_res_df['time_diff'] = city_res_df['creation_time'].diff().dt.fillna(0)\n",
    "  city_res_df['time_diff'] = city_res_df['creation_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "\n",
    "  # Example duration of each entry (assuming hourly intervals if not directly calculable)\n",
    "  # If your intervals are consistent but not hourly, adjust 'time_diff' calculation accordingly\n",
    "  #hourly_interval_seconds = 3600\n",
    "  #city_res_df['time_diff'].replace(0, hourly_interval_seconds, inplace=True)\n",
    "\n",
    "  # Initialize a dictionary to store the total duration each ad campaign runs in each city\n",
    "  duration_per_ad_per_city = {city: {} for city in city_res_df.columns if city != 'creation_time' and city != 'time_diff'}\n",
    "\n",
    "  # Calculate the total duration for each ad type in each city\n",
    "  for city in duration_per_ad_per_city.keys():\n",
    "      # Group by ad type and sum the duration\n",
    "      ad_durations = city_res_df.groupby(city)['time_diff'].sum()\n",
    "      duration_per_ad_per_city[city] = ad_durations.to_dict()\n",
    "\n",
    "  # Calculate the total observed time\n",
    "  total_time = city_res_df['time_diff'].sum()\n",
    "\n",
    "  # Calculate the percentage of total time for each ad campaign in each city\n",
    "  percentage_per_ad_per_city = {city: {ad: (duration / total_time * 100) for ad, duration in durations.items()} for city, durations in duration_per_ad_per_city.items()}\n",
    "\n",
    "  # Example output\n",
    "  # Combine the two dictionaries into a single dictionary\n",
    "  summary_stats = {\n",
    "    'Duration (in hours)': duration_per_ad_per_city,\n",
    "    'Percentage of total time': percentage_per_ad_per_city\n",
    "  }\n",
    "  return summary_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbdev this, github and streamlit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat_lon': (40.7128, -74.006), 'time_zone': 'America/New_York', 'local_peak_start': '2024-03-21 10:00:00', 'local_peak_end': '2024-03-21 17:00:00'}\n",
      "{'lat_lon': (28.6139, 77.209), 'time_zone': 'Asia/Kolkata', 'local_peak_start': '2024-03-21 19:30:00', 'local_peak_end': '2024-03-22 02:30:00'}\n",
      "Timezone offset for latitude 40.7128, longitude -74.006 is -4.0 hours.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "if IN_COLAB:\n",
    "    from nbdev.export import notebook2script\n",
    "    notebook2script()\n",
    "else:\n",
    "    import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
